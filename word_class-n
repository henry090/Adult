library(keras)
library(tidyquant)


activity_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip"
temp <- tempfile()
download.file(activity_url, temp)
unzip(temp, "sentiment labelled sentences/amazon_cells_labelled.txt")

df1 <- readLines('amazon_cells_labelled.txt')

as.data.frame(do.call(rbind, strsplit(df1, split="\t")), stringsAsFactors=FALSE) %>% 
  `colnames<-`(c('Review','Rate')) -> df1

total_reviews <- df1


sentences <- as.list(total_reviews$Review)
sapply(sentences, tolower) %>% as.list()->sentences

space_before_punct <- function(sentence) {
  str_replace_all(sentence, "([[?.!,],.])|[[:punct:]]|\\s|\\t", " \\1")
}

replace_special_chars <- function(sentence) {
  str_replace_all(sentence, "[^A-z]", " ")
}

add_tokens <- function(sentence) {
  paste0("<start> ", sentence, " <stop>")
}


add_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)

preprocess_sentence <- compose(add_tokens,
                               str_squish,
                               replace_special_chars,
                               space_before_punct)



word_pairs <- map(sentences, preprocess_sentence)




create_index <- function(sentences) {
  unique_words <- sentences %>% unlist() %>% paste(collapse = " ") %>%
    str_split(pattern = " ") %>% .[[1]] %>% unique() %>% sort()
  index <- data.frame(
    word = unique_words,
    index = 1:length(unique_words),
    stringsAsFactors = FALSE
  )
  index
}

word2index <- function(word, index_df) {
  index_df[index_df$word == word, "index"]
}
index2word <- function(index, index_df) {
  index_df[index_df$index == index, "word"]
}

src_index <- create_index(map(word_pairs, ~ .[[1]]))

src_index %>% add_row(word = "<PAD>", index = 0) %>% arrange(index)->src_index


sentence2digits <- function(sentence, index_df) {
  map((sentence %>% str_split(pattern = " "))[[1]], function(word)
    word2index(word, index_df))
}

sentlist2diglist <- function(sentence_list, index_df) {
  map(sentence_list, function(sentence)
    sentence2digits(sentence, index_df))
}

src_diglist <-
  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)
src_maxlen <- map(src_diglist, length) %>% unlist() %>% max()
src_matrix <-
  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = "post",value = src_index %>% filter(word == "<PAD>") %>% 
                  select(index) %>% pull())

total_reviews$Rate %>% as.integer() -> total_reviews$Rate

train<-src_matrix[201:1000,1:33]
train_labels<-total_reviews$Rate[201:1000]

test<-src_matrix[1:200,1:33]
test_labels<-total_reviews$Rate[1:200]


# input shape is the vocabulary count used for the movie reviews (10,000 words)
vocab_size <- 1816

#-----------------------------------
set.seed(30)
model <- keras_model_sequential()
model %>% 
  layer_embedding(input_dim = vocab_size, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu",
  kernel_regularizer = regularizer_l2(l=0.1)) %>%
  layer_gaussian_dropout(rate = 0.2) %>% 
  layer_dense(units = 1, activation = "sigmoid",kernel_regularizer = regularizer_l2(l=0.004)) 


model %>% compile(
  optimizer = 'adam',
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)

model %>% summary()


history <- model %>% fit(
  train,
  train_labels,
  epochs = 90,
  batch_size = 50,
  validation_data = list(test, test_labels),
  verbose=1
)

results <- model %>% evaluate(test, test_labels)
results

library(yardstick)
# Predicted Class
yhat_keras_class_vec <- predict_classes(object = model, x = as.matrix(test)) %>%
  as.vector()

# Predicted Class Probability
yhat_keras_prob_vec  <- predict_proba(object = model, x = as.matrix(test)) %>%
  as.vector()

# Format test data and predictions for yardstick metrics
estimates_keras_tbl <- tibble(
  truth      = as.factor(test_labels) %>% fct_recode(positive = "1", negative = "0"),
  estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(positive = "1", negative = "0"),
  class_prob = yhat_keras_prob_vec
)

estimates_keras_tbl


options(yardstick.event_first = FALSE)


# Confusion Table
estimates_keras_tbl %>% conf_mat(truth, estimate)

estimates_keras_tbl %>% conf_mat(truth, estimate)->LLL


LLL$table %>% fourfoldplot(conf.level = 0, color = c("#ed3b3b", "#0099ff"),
                           margin = 1,main = paste('Keras',
                                                   round(sum(diag(.))/sum(.)*100,0),"%",sep = ' '))






model_type.keras.engine.sequential.Sequential <- function(x, ...) {
  return("classification")
}

# Setup lime::predict_model() function for keras
predict_model.keras.engine.sequential.Sequential <- function(x, newdata, type, ...) {
  pred <- predict_proba(object = x, x = as.matrix(newdata))
  return(data.frame(Positive = pred, Negative = 1 - pred))
}

library(lime)
# Test our predict_model() function
predict_model(x = model, newdata = test, type = 'raw') %>%
  tibble::as_tibble()

decode_review <- function(text){
  paste(map(text, function(number) src_index %>%
              filter(word == number) %>%
              select(index) %>% 
              pull() ),
        collapse = " ") %>% str_split(.,pattern = ' ',simplify = T)  %>% as.integer() %>% t() %>% as.matrix()
  
}

word_pairs %>% unlist() %>% as.data.frame() %>% mutate_if(is.character,as.factor) ->now



str_split(now$., " ", simplify = TRUE)->new

decode_review(new[1:1,1:33])

explainer <- lime::lime(
  x              = new[1:1,1:23], 
  model          = model,
  preprocess = decode_review)


explanation <- lime::explain(
  new[1:1,1:23], 
  explainer    = explainer, 
  n_labels     = 1, 
  n_features   = 3,
)







